6.824 2020 Lecture 1: Introduction

6.824 分布式系统工程

何谓分布式系统？
    多个共同协作的计算机，大型网站的存储结构，MapReduce，点对点共享等
    很多至关重要的基础架构也是分布式的。

为何人们需要构筑分布式系统？
    - 可以通过并行来扩容
    - 可以通过主从复制来提高容错
    - 可以把计算量物理上移近外部实体??
    - 可以通过隔离来实现安全性

不足：
    - 很多并发部分，以及复杂的相互作用
    - 必须能应对局部故障
    - 想实现潜在性能依旧很棘手

学习这门课的理由：
    + 有趣。 --复杂的问题以及很棒的解决颁发
    + 被现实的系统所使用。 --应大型网站的兴起而产生
    + 活跃的研究领域。 --重要而尚未解决的问题
    + 亲手实践。 --你可以在labs里自己构建真正的(分布式)系统


课程结构
链接：http://pdos.csail.mit.edu/6.824

课程教员：
    - 讲师：Robert Morris
    - 助教: Anish Athalye
    - 助教：Aakriti Shroff
    - 助教：Favyen Bastani
    - 助教：Tossaporn Saengja

课程构成：
    重要的想法，论文讨论以及实验将会被录成视频并且线上可观看

论文：
    研究论文，一些经典的or新的问题以及想法、实现细节和评价。
    很多演讲注重在论文上
    上课前请研究下论文
    每篇论文都有个简短的小问题让你回答，并且我们会让你给出关于论文的问题
    上课前一天午夜截止前提交你的问题和答案

测验：
    课堂上进行期中测试
    期末考试周进行期末测试
    大多数都是关于论文和实验的测试

实验：
    目标： 对一些重点技术有更深的理解
    目标： 有分布式编程的经验
    星期五开始的一周内为第一次实验的时间
    自那开始每周一个实验

实验1：MapReduce
实验2：使用Raft算法的主从复制来保证容错
实验3：容错性键值数据库
实验4：分片的键值数据库

在最后有可选的2至3人的小组的毕业设计。
毕业设计用来代替实验4。
由你想出一个项目并且向我们阐述它。
截止日期前给出代码、简评以及小demo。

实验成绩取决于有多少测试case你可以通过，
我们会给你测试数据以便你自己清楚做的如何。

实验中调试可能很花时间，
早点开始，
经常来助教的办公室，
在Piazza上提问

主题

这是一门关于应用的基础设施的课程。
    - 存储
    - 通信
    - 计算

宏远的目标：隐藏了分布式复杂性的抽象概念，
会有大量topic反复在研究中被提出。

Topic：实现RPC，线程，并发控制

Topic：性能
  目标：可伸缩的吞吐
    N个服务器 -> 基于并行的CPU、disk、net上的N倍的吞吐量
    [图解：用户，应用服务器，存储服务器]
    因此处理更多的负载只需要购买更多的机器，
    而非由昂贵的程序员重新设计系统。
    当你可以把工作量拆分为少量交互作用时尤为有效。
  N增大时扩展变得困难：
    负载不均衡，游离的服务器，N中最慢的延迟。
    不可并行的代码：初始化，交互。
    由共享资源比如网络而滋生的瓶颈。
  一些性能问题很难简单用扩展来解决：
    比如对于单个用户请求的快速相应时间
    比如所有用户都想更新同一份数据
    这些都要求更良好的设计而非只是加更多的机器

Topic：容错
  1000+的服务器，庞大的网络结构 -> 总会有某些机器宕机
  我们并不想在应用层面暴露这些问题
  通常所希望的是：
    可用性 - 就算报错应用也行继续运行
    可恢复性 - 当错误修复应用可以恢复正常
  大胆的想法： 服务器副本
    如果某台服务器宕机了，系统可以利用其他机器继续运行下去

Topic：一致性
  通用的基础设施需要定义明确的行为。
    比如Get(k)返回最近一次Put(k,v)的值。
  达到良好的表现是很难的。
    服务器“副本”很难实现一致。
    客户端可能在多步更新时中途奔溃。
    服务端可能在执行之后返回响应前崩溃。
    网络分区可能让存活的服务器看起来是宕机的，类似于"split brain"的风险(??)
  一致性和性能不可兼得：
    强一致性需要通信来维持。
      例如：Get()必须检查最近的一次Put()
    许多设计者只提供了弱一致性，以此为代价来实现更高的速度。
      例如：Get()并不一定和最近的一次Put()保持一致
      对于程序员来说会很痛苦但应该是很不错的折衷。
  许多设计点在一致性/性能方面是可行的(??)

案例研究: MapReduce

以MapReduce(MR)作为案例来研究一下
  一幅很不错的关于6.824主题的图解
  极其有影响力
  Lab 1的重点

MapReduce概览：
  上下文：数TB的数据集上花费数小时的计算
    比如：建立检索索引、排序，或者分析web流量的结构
    只在用上千台电脑的情况下
    应用并非由分布式系统专家所写
  最终目标：对于非专业程序员也容易实现
  程序员只需要定义Map和Reduce函数
    经常只是简单的顺序代码即可运作
  MR注重并且隐藏了分布式的所有方面

MapReduce很容易扩展：
  N个worker可以带来N倍的吞吐量
    Maps函数可以并行执行，因为它们无需交互，这一点与Reduce一样
  因此你可以通过加更多的机器来实现更大的吞吐量

MapReduce隐藏了很多细节：
  发送app代码给服务器
  跟踪哪些任务已完成
  把数据从Maps发给Reduces
  服务器之间负载均衡
  故障恢复

但同时MapReduce也是限制了app的一些行为：
  不能有交互或者状态信息(除非直接的结果输出)
  不能有迭代，也不能有多级管道运算
  不能有实时或流式的处理

输入输出存储在GFS cluster文件系统：
  MR需要大量并行的输入输出吞吐量
  GFS把文件以64MB块的形式把存在各个服务器
    Maps并行读
    Reduces并行写
  GFS同时为每个文件创建了位于2到3个服务器上的副本

限制性能的可能因素：
  关注这个可以进一步优化
  CPU？内存？磁盘？网络？
  2004年作者那时受限于网络容量
    MR在网络上收发哪些数据
      - Maps从GFS读取输入
      - Reduces读取Map的输出
      - Reduces向GFS写输出结果
    关键词：服务器，树形网络交换
    在MR的all-to-all洗牌下，一半的流量经过根节点
    论文里根节点：100-200Gb/s，总共1800台机器，因此算下来55Mb/s/台机器
    55对于硬盘或RAM速度来很小
    现在网络和根节点相对CPU/disk来讲比那时快得多

一些细节(论文的配图1)：
  Master节点试图在保存输入的GFS服务器上直接运行Map任务
    所有机器都运行GFS和MR worker两个任务
    因此input是直接从本地磁盘读取，而非经过网络。
  中间数据只走一次网络。
    Map worker直接写本地磁盘
    Reduce worker直接从Map woker读取而非通过GFS
  中间数据被分区为多个包含keys的文件。
    R(Reduce的数量?)比keys数量小很多。
    大规模网络传输更有效率。

MR如何实现负载均衡：
  如果N-1个服务器不得不等待一个慢服务器执行完毕将会非常浪费和缓慢。
  但一些任务似乎是比别的话费更长时间。
  解决方案：
    Master把新的任务分发给已经完成之前任务的worker。
    这样应该不会有非常耗时的任务支配了任务的完成时间。
    快速的服务器比缓慢的执行更多的任务，达到同时完成的目标。

关于容错：
  例如：如果MR任务里一个worker宕机会怎样
  我们所希望的是完全向应用层程序员隐藏这些故障。
  MR是否必须从头重新跑整个任务？
    为什么不需要？
  MR只需要重新跑那些失败了的Map()s和Reduce()s。
    假设MR跑了一个Map任务两次，一个Reduce看到了它的第一次输出，
    另一个Reduce看到了第二次输出？
    必须重新执行后有同样的输出才能保证正确性
    因此Map和Reduce必须是纯确定性函数(无状态??)。
      它们只着重于传入的参数。
      无状态，无文件IO，无交互，也无外部通信
    但如果想允许非功能性的Map和Reduce怎么办？
      Worker失败将需要整个任务重新执行，
      或者你需要创建全局同步的检查点。

worker宕机恢复的细节：
  Map worker宕机：
    master通知worker不再对pings响应
    master知道在那个worker上跑了哪个Map任务
      那些任务的中间结果丢失了，因此必须重新跑一遍
      master通知其他worker跑这些任务
    当然如果Reduces已经取到了这些中间结果可以不需要重跑
  Reduce worker宕机：
    已经完成的任务无需关注 --已经保存在GFS中
    master在其它worker上重跑未完成的任务

其它故障/问题：
  - 如果master给了两台worker同样的Map任务会怎样？
    也许master会错误地判断其中一台worker死机了，
    它会只通知Reduce worker其中一台有数据。
  - 如果master给了两台worker同样的Reduce任务会怎样？
    它们会在GFS写重复的输出。
    原子性的GFS重命名可以防止混乱；一个已完成的文件将会可见
  - 如果某个worker非常慢？
    可能由于不可靠的硬件
    master会在最后几个任务时开启二次执行。
  - 如果一个worker由于硬件/软件故障产生了错误的输出会怎样？
    MR是运行在失败即停的CPU和软件前提下，这个难以处理。
  - master宕机会怎样？
    (cluster？replica？)

当前情形：
  巨大影响力的(Hadoop，Spark)
  很可能已经不在Google里应用了。
    由Flume/FlumeJava代替。
    GFS由Colossus以及BigTable代替。

结论：
  MapReduce以一己之力使得大规模集群计算大众化变得可能。
  - 不是最高效或最灵活的
  - 但易于扩展
  - 易于编程， 故障和数据传送都被隐藏起来了
  它们属于实践中优秀的trade-off策略。
  后面的课程我们会看到更先进的理念。
  实验愉快！

